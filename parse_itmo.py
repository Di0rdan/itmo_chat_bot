#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import re
from bs4 import BeautifulSoup
from typing import Dict, List, Any
import argparse
import requests
import os


def parse_itmo_program(html_file_path: str) -> Dict[str, Any]:
    """
    –ü–∞—Ä—Å–∏—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—É –º–∞–≥–∏—Å—Ç–µ—Ä—Å–∫–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã –ò–¢–ú–û –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    """
    with open(html_file_path, 'r', encoding='utf-8') as file:
        html_content = file.read()
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è JSON
    program_data = {
        "university": "–ò–¢–ú–û",
        "program_type": "–º–∞–≥–∏—Å—Ç—Ä–∞—Ç—É—Ä–∞",
        "year": 2025
    }
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–≥—Ä–∞–º–º–µ
    program_data["basic_info"] = extract_basic_info(soup)
    
    # –ú–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–≥—Ä–∞–º–º—ã
    program_data["program_manager"] = extract_program_manager(soup)
    
    # –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏
    program_data["social_networks"] = extract_social_networks(soup)
    
    # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏
    program_data["study_directions"] = extract_study_directions(soup)
    
    # –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã
    program_data["program_description"] = extract_program_description(soup)
    
    # –ü–∞—Ä—Ç–Ω–µ—Ä—ã
    program_data["partners"] = extract_partners(soup)
    
    # –ö–æ–º–∞–Ω–¥–∞ –ø—Ä–æ–≥—Ä–∞–º–º—ã
    program_data["team"] = extract_team_info(soup)
    
    # –ö–∞—Ä—å–µ—Ä–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
    program_data["career"] = extract_career_info(soup)
    
    # –ö–æ–º–ø–∞–Ω–∏–∏-—Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª–∏
    program_data["employers"] = extract_employers(soup)
    
    # –û—Ç–∑—ã–≤—ã –≤—ã–ø—É—Å–∫–Ω–∏–∫–æ–≤
    program_data["alumni_reviews"] = extract_alumni_reviews(soup)
    
    # –î–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤
    program_data["achievements"] = extract_achievements(soup)
    
    # –ü–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ
    program_data["admission"] = extract_admission_info(soup)
    
    # –°—Ç–∏–ø–µ–Ω–¥–∏–∏
    program_data["scholarships"] = extract_scholarships(soup)
    
    # –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
    program_data["international_opportunities"] = extract_international_opportunities(soup)
    
    # FAQ
    program_data["faq"] = extract_faq(soup)
    
    # –ü–æ—Ö–æ–∂–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã
    program_data["similar_programs"] = extract_similar_programs(soup)
    
    return program_data

def extract_basic_info(soup: BeautifulSoup) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–≥—Ä–∞–º–º–µ"""
    basic_info = {}
    
    # –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã
    title_elem = soup.find('h1', class_='Information_information__header__fab3I')
    if title_elem:
        basic_info["title"] = title_elem.get_text(strip=True)
    
    # –ò–Ω—Å—Ç–∏—Ç—É—Ç
    institute_elem = soup.find('div', class_='Information_information__link__cfN2l')
    if institute_elem:
        basic_info["institute"] = institute_elem.get_text(strip=True)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–æ–≥—Ä–∞–º–º—ã –∏–∑ –∫–∞—Ä—Ç–æ—á–µ–∫
    cards = soup.find_all('div', class_='Information_card__rshys')
    for card in cards:
        header = card.find('div', class_='Information_card__header__6PpVf')
        text = card.find('div', class_='Information_card__text__txwcx')
        
        if header and text:
            header_text = header.get_text(strip=True).lower()
            value = text.get_text(strip=True)
            
            if '—Ñ–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è' in header_text:
                basic_info["study_form"] = value
            elif '–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å' in header_text:
                basic_info["duration"] = value
            elif '—è–∑—ã–∫ –æ–±—É—á–µ–Ω–∏—è' in header_text:
                basic_info["language"] = value
            elif '—Å—Ç–æ–∏–º–æ—Å—Ç—å' in header_text:
                basic_info["cost_per_year"] = value
            elif '–æ–±—â–µ–∂–∏—Ç–∏–µ' in header_text:
                basic_info["dormitory"] = value == "–¥–∞"
            elif '–≤–æ–µ–Ω–Ω—ã–π —É—á–µ–±–Ω—ã–π —Ü–µ–Ω—Ç—Ä' in header_text:
                basic_info["military_center"] = value == "–¥–∞"
            elif '–≥–æ—Å. –∞–∫–∫—Ä–µ–¥–∏—Ç–∞—Ü–∏—è' in header_text:
                basic_info["state_accreditation"] = value == "–¥–∞"
            elif '–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏' in header_text:
                basic_info["additional_opportunities"] = value
    
    return basic_info

def extract_program_manager(soup: BeautifulSoup) -> Dict[str, str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ–Ω–µ–¥–∂–µ—Ä–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã"""
    manager_info = {}
    
    manager_section = soup.find('div', class_='Information_manager__XZOI3')
    if manager_section:
        # –ò–º—è
        name_elem = manager_section.find('div', class_='Information_manager__name__ecPmn')
        if name_elem:
            name_text = name_elem.get_text(strip=True)
            # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç –±—ã—Ç—å alt —Ç–µ–∫—Å—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            name_parts = name_text.split('\n')
            manager_info["name"] = name_parts[-1] if name_parts else name_text
        
        # –ö–æ–Ω—Ç–∞–∫—Ç—ã
        contacts = manager_section.find_all('div', class_='Information_manager__contact__1fPAH')
        for contact in contacts:
            link = contact.find('a')
            if link:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                if href.startswith('mailto:'):
                    manager_info["email"] = text
                elif href.startswith('tel:'):
                    manager_info["phone"] = text
        
        # –§–æ—Ç–æ
        photo_elem = manager_section.find('img')
        if photo_elem:
            manager_info["photo_url"] = photo_elem.get('src', '')
    
    return manager_info

def extract_social_networks(soup: BeautifulSoup) -> List[Dict[str, str]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã"""
    social_networks = []
    
    social_section = soup.find('div', class_='Information_socials__JpNII')
    if social_section:
        links = social_section.find_all('a', class_='Information_socials__link___eN3E')
        for link in links:
            social_networks.append({
                "name": link.get_text(strip=True),
                "url": link.get('href', '')
            })
    
    return social_networks

def extract_study_directions(soup: BeautifulSoup) -> List[Dict[str, Any]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏"""
    directions = []
    
    direction_items = soup.find_all('div', class_='Directions_table__item__206L0')
    for item in direction_items:
        direction = {}
        
        # –ö–æ–¥ –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        header = item.find('div', class_='Directions_table__header__qV8_J')
        if header:
            code_elem = header.find('p')
            name_elem = header.find('h5')
            if code_elem:
                direction["code"] = code_elem.get_text(strip=True)
            if name_elem:
                direction["name"] = name_elem.get_text(strip=True)
        
        # –ú–µ—Å—Ç–∞
        places_info = item.find('div', class_='Directions_table__info__HQR4Y')
        if places_info:
            places = places_info.find_all('div', class_='Directions_table__places__RWYBT')
            direction["places"] = {}
            
            for place in places:
                span = place.find('span')
                p = place.find('p')
                if span and p:
                    place_type = p.get_text(strip=True)
                    place_count = span.get_text(strip=True)
                    
                    if '–±—é–¥–∂–µ—Ç–Ω—ã—Ö' in place_type:
                        direction["places"]["budget"] = int(place_count)
                    elif '—Ü–µ–ª–µ–≤–∞—è' in place_type:
                        direction["places"]["targeted"] = int(place_count)
                    elif '–∫–æ–Ω—Ç—Ä–∞–∫—Ç–Ω—ã—Ö' in place_type:
                        direction["places"]["contract"] = int(place_count)
        
        directions.append(direction)
    
    return directions

def extract_program_description(soup: BeautifulSoup) -> Dict[str, str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã"""
    description = {}
    
    about_section = soup.find('div', class_='AboutProgram_aboutProgram__textBlock__LpASa')
    if about_section:
        # –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
        lead_elem = about_section.find('span', class_='AboutProgram_aboutProgram__lead__SBgI1')
        if lead_elem:
            description["summary"] = lead_elem.get_text(strip=True)
        
        # –ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
        desc_elem = about_section.find('span', class_='AboutProgram_aboutProgram__description__Bf9LA')
        if desc_elem:
            description["full_description"] = desc_elem.get_text(strip=True)
    
    return description

def extract_partners(soup: BeautifulSoup) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º—ã"""
    partners = []
    
    partner_cards = soup.find_all('div', class_='Partners_partners__card__STOzK')
    for card in partner_cards:
        img = card.find('img')
        if img:
            alt_text = img.get('alt', '')
            src = img.get('src', '')
            if src:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–∞—Ä—Ç–Ω–µ—Ä–∞ –∏–∑ –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
                filename = src.split('/')[-1].split('.')[0]
                partners.append(filename.replace('_', ' ').title())
    
    return partners

def extract_team_info(soup: BeautifulSoup) -> Dict[str, str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–º–∞–Ω–¥–µ"""
    team_section = soup.find('div', class_='Team_team__bXtHu')
    if team_section:
        title_elem = team_section.find('h2')
        if title_elem:
            return {"section_title": title_elem.get_text(strip=True)}
    return {}

def extract_career_info(soup: BeautifulSoup) -> Dict[str, str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞—Ä—å–µ—Ä–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö"""
    career = {}
    
    career_section = soup.find('div', class_='Career_career__Fc883')
    if career_section:
        h5_elem = career_section.find('h5')
        if h5_elem:
            career["description"] = h5_elem.get_text(strip=True)
    
    return career

def extract_employers(soup: BeautifulSoup) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–º–ø–∞–Ω–∏–π-—Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª–µ–π"""
    employers = []
    
    job_cards = soup.find_all('div', class_='Job_job__card__lGEpQ')
    for card in job_cards:
        img = card.find('img')
        if img:
            src = img.get('src', '')
            if src:
                filename = src.split('/')[-1].split('.')[0]
                # –û—á–∏—â–∞–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
                company_name = filename.replace('_', ' ').replace('1', '').replace('2', '').title()
                employers.append(company_name)
    
    return employers

def extract_alumni_reviews(soup: BeautifulSoup) -> List[Dict[str, str]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Ç–∑—ã–≤—ã –≤—ã–ø—É—Å–∫–Ω–∏–∫–æ–≤"""
    reviews = []
    
    review_items = soup.find_all('li', class_='Slider_slider__item__DCIiq')
    for item in review_items:
        review = {}
        
        # –¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞
        text_elem = item.find('div', class_='Slider_slider__textForDesktop__ar2Q4')
        if text_elem:
            review["text"] = text_elem.get_text(strip=True)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤—ã–ø—É—Å–∫–Ω–∏–∫–µ
        name_elem = item.find('p', class_='Slider_slider__name__K8ZO_')
        year_elem = item.find('span', class_='Slider_slider__position__jv528')
        
        if name_elem:
            review["name"] = name_elem.get_text(strip=True)
        if year_elem:
            review["graduation_year"] = year_elem.get_text(strip=True)
        
        if review:
            reviews.append(review)
    
    return reviews

def extract_achievements(soup: BeautifulSoup) -> List[Dict[str, str]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤"""
    achievements = []
    
    achievement_cards = soup.find_all('div', class_='Achievements_achievements__card__yolZ5')
    for card in achievement_cards:
        achievement = {}
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        h5_elem = card.find('h5')
        if h5_elem:
            achievement["title"] = h5_elem.get_text(strip=True)
        
        # –°—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏
        link_elem = card.find('a')
        if link_elem:
            achievement["url"] = link_elem.get('href', '')
        
        # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        img_elem = card.find('img')
        if img_elem:
            achievement["image_url"] = img_elem.get('src', '')
        
        achievements.append(achievement)
    
    return achievements

def extract_admission_info(soup: BeautifulSoup) -> Dict[str, List[Dict[str, str]]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–∏"""
    admission = {"ways_to_apply": []}
    
    accordion_items = soup.find_all('div', class_='Accordion_accordion__item__A6W5t')
    for item in accordion_items:
        way = {}
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ —Å–ø–æ—Å–æ–±–∞ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è
        title_elem = item.find('h5')
        if title_elem:
            way["name"] = title_elem.get_text(strip=True)
        
        # –û–ø–∏—Å–∞–Ω–∏–µ
        info_elem = item.find('div', class_='Accordion_accordion__info__wkCQC')
        if info_elem:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç, –∏—Å–∫–ª—é—á–∞—è —Å—Å—ã–ª–∫–∏
            desc_div = info_elem.find('div')
            if desc_div:
                way["description"] = desc_div.get_text(strip=True)
            
            # –°—Å—ã–ª–∫–∞ –Ω–∞ –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏
            link_elem = info_elem.find('a')
            if link_elem:
                way["details_url"] = link_elem.get('href', '')
        
        if way.get("name"):
            admission["ways_to_apply"].append(way)
    
    return admission

def extract_scholarships(soup: BeautifulSoup) -> List[Dict[str, str]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç–∏–ø–µ–Ω–¥–∏—è—Ö"""
    scholarships = []
    
    scholarship_items = soup.find_all('a', class_='Scholarship_item__cowlU')
    for item in scholarship_items:
        scholarship = {}
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∏–ø–µ–Ω–¥–∏–∏
        h5_elem = item.find('h5')
        if h5_elem:
            scholarship["name"] = h5_elem.get_text(strip=True)
        
        # –°—É–º–º–∞
        h4_elem = item.find('h4')
        if h4_elem:
            scholarship["amount"] = h4_elem.get_text(strip=True)
        
        # –°—Å—ã–ª–∫–∞
        scholarship["url"] = item.get('href', '')
        
        scholarships.append(scholarship)
    
    return scholarships

def extract_international_opportunities(soup: BeautifulSoup) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö"""
    opportunities = {}
    
    opp_section = soup.find('div', class_='Opportunities_opportunities__P3Pj8')
    if opp_section:
        # –û—Å–Ω–æ–≤–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
        text_elem = opp_section.find('p', class_='Opportunities_opportunities__text__axjuV')
        if text_elem:
            opportunities["description"] = text_elem.get_text(strip=True)
        
        # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏–∑ –∞–∫–∫–æ—Ä–¥–µ–æ–Ω–∞
        opportunities["programs"] = []
        accordion_items = opp_section.find_all('div', class_='Accordion_accordion__item__A6W5t')
        
        for item in accordion_items:
            program = {}
            
            title_elem = item.find('h5')
            if title_elem:
                program["name"] = title_elem.get_text(strip=True)
            
            desc_elem = item.find('div', class_='Accordion_accordion__info__wkCQC')
            if desc_elem:
                desc_div = desc_elem.find('div')
                if desc_div:
                    program["description"] = desc_div.get_text(strip=True)
            
            if program.get("name"):
                opportunities["programs"].append(program)
    
    return opportunities

def extract_faq(soup: BeautifulSoup) -> List[Dict[str, str]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ –≤–æ–ø—Ä–æ—Å—ã"""
    faq_list = []
    
    # –ò—â–µ–º FAQ —Å–µ–∫—Ü–∏—é
    faq_section = soup.find('h2', class_='Faq_title__aGKyL')
    if faq_section:
        parent = faq_section.find_parent()
        if parent:
            accordion_items = parent.find_all('div', class_='Accordion_accordion__item__A6W5t')
            
            for item in accordion_items:
                faq_item = {}
                
                # –í–æ–ø—Ä–æ—Å
                question_elem = item.find('h5')
                if question_elem:
                    faq_item["question"] = question_elem.get_text(strip=True)
                
                # –û—Ç–≤–µ—Ç
                answer_elem = item.find('div', class_='Accordion_accordion__info__wkCQC')
                if answer_elem:
                    answer_div = answer_elem.find('div')
                    if answer_div:
                        faq_item["answer"] = answer_div.get_text(strip=True)
                
                if faq_item.get("question"):
                    faq_list.append(faq_item)
    
    return faq_list

def extract_similar_programs(soup: BeautifulSoup) -> List[Dict[str, str]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã"""
    similar_programs = []
    
    program_items = soup.find_all('a', class_='SimilarPrograms_programs__item__u2gRI')
    for item in program_items:
        program = {}
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã
        h5_elem = item.find('h5')
        if h5_elem:
            program["name"] = h5_elem.get_text(strip=True)
        
        # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏
        direction_elem = item.find('p')
        if direction_elem:
            program["direction"] = direction_elem.get_text(strip=True)
        
        # –°—Å—ã–ª–∫–∞
        program["url"] = item.get('href', '')
        
        similar_programs.append(program)
    
    return similar_programs


def load_html_repr(program: str, output_file: str) -> str:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç HTML –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã"""

    url = f'https://abit.itmo.ru/program/master/{program}'
    response = requests.get(url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')

        # –£–¥–∞–ª–∏–º –Ω–µ–Ω—É–∂–Ω—ã–µ —Ç–µ–≥–∏
        for tag in soup(['script', 'style', 'svg', 'noscript', 'iframe', 'meta', 'link']):
            tag.decompose()

        # –£–¥–∞–ª–∏–º —Å–∫—Ä—ã—Ç—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Å—Ç–∏–ª—é
        for tag in soup.find_all(style=True):
            if 'display:none' in tag['style'] or 'visibility:hidden' in tag['style']:
                tag.decompose()

        # –£–¥–∞–ª–∏–º —Å–∫—Ä—ã—Ç—ã–µ –∫–ª–∞—Å—Å—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        hidden_classes = ['sr-only', 'visually-hidden']
        for cls in hidden_classes:
            for tag in soup.select(f'.{cls}'):
                tag.decompose()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π HTML
        with open(output_file, 'w', encoding='utf-8') as file:
            file.write(str(soup.prettify()))

        print(f"–û—á–∏—â–µ–Ω–Ω–∞—è HTML-—Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{output_file}'")
    else:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {response.status_code}")


def main(program: str):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    html_file = f"clean_page_{program}.html"
    output_file = f"itmo_{program}_program.json"

    load_html_repr(program, html_file)

    try:
        # –ü–∞—Ä—Å–∏–º HTML —Ñ–∞–π–ª
        program_data = parse_itmo_program(html_file)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON —Ñ–∞–π–ª
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(program_data, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
        print(f"üìä –ò–∑–≤–ª–µ—á–µ–Ω–æ:")
        print(f"   - –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–≥—Ä–∞–º–º–µ")
        print(f"   - {len(program_data.get('study_directions', []))} –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏")
        print(f"   - {len(program_data.get('partners', []))} –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤")
        print(f"   - {len(program_data.get('employers', []))} –∫–æ–º–ø–∞–Ω–∏–π-—Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª–µ–π")
        print(f"   - {len(program_data.get('scholarships', []))} –≤–∏–¥–æ–≤ —Å—Ç–∏–ø–µ–Ω–¥–∏–π")
        print(f"   - {len(program_data.get('faq', []))} –≤–æ–ø—Ä–æ—Å–æ–≤ FAQ")
        print(f"   - {len(program_data.get('admission', {}).get('ways_to_apply', []))} —Å–ø–æ—Å–æ–±–æ–≤ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è")

        os.remove(html_file)

    except FileNotFoundError:
        print(f"‚ùå –§–∞–π–ª {html_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='–ü–∞—Ä—Å–∏—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—É –º–∞–≥–∏—Å—Ç–µ—Ä—Å–∫–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã –ò–¢–ú–û')
    parser.add_argument("--program", type=str, required=True, help="ai/ai_product")
    program = parser.parse_args().program
    main(program)
